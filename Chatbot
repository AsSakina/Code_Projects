import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import streamlit as st
nltk.download('stopwords')
nltk.download('wordnet')

#try_encodings = ['utf-8', 'latin-1', 'ISO-8859-1', 'UTF-16']

#for encoding in try_encodings:
    #try:
        #with open('/Users/soukeynafaye/Downloads/Enfant_Psycho.txt', 'r', encoding='utf-8') as f:
            #data = f.read()
        #break
    #except UnicodeDecodeError:
        #continue

import chardet

with open('/Users/soukeynafaye/Downloads/Enfant_Psycho.txt', 'rb') as f:
    data = f.read()

result = chardet.detect(data)
encoding = result['encoding']

with open('/Users/soukeynafaye/Downloads/Enfant_Psycho.txt', 'r', encoding=encoding) as f:
    data = f.read()


# Créer un dictionnaire pour stocker les paires question-réponse
qa_pairs = {}
question = ""
for line in data:
    line = line.strip()
    if line.endswith('?'):
        # Si la ligne se termine par '?', alors c'est une question
        question = line
    elif line.endswith('.'):
        # Si la ligne se termine par '.', alors c'est une réponse
        if question:
            qa_pairs[question] = line
            question = ""


# Tokenize the text into sentences
sentences = sent_tokenize(data)
# Define a function to preprocess each sentence
def preprocess(sentence):
    # Tokenize the sentence into words
    words = word_tokenize(sentence)
    # Remove stopwords and punctuation
    words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word not in string.punctuation]
    # Lemmatize the words
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    return words


# Preprocess each sentence in the text
corpus = [preprocess(sentence) for sentence in sentences]

# Define a function to find the most relevant sentence given a query
def get_most_relevant_sentence(query):
    # Preprocess the query
    query = preprocess(query)
    # Compute the similarity between the query and each sentence in the text
    max_similarity = 0
    most_relevant_sentence = ""
    for sentence in corpus:
        similarity = len(set(query).intersection(sentence)) / float(len(set(query).union(sentence)))
        if similarity > max_similarity:
            max_similarity = similarity
            most_relevant_sentence = " ".join(sentence)
    return most_relevant_sentence

def chatbot(question):
    #Find the most relevant sentence
    most_relevant_sentence = get_most_relevant_sentence(question)
    # Return the answer
    return most_relevant_sentence

def chatbot(question):
    # Find the most relevant sentence
    most_relevant_sentence = get_most_relevant_sentence(question)

    # Return the answer (You can customize the response based on the most relevant sentence found)
    if most_relevant_sentence:
        response = "Le chatbot dit: " + most_relevant_sentence
    else:
        response = "Le chatbot dit: Je suis désolé, je n'ai pas trouvé d'informations pertinentes pour votre question."
    return response

# Create a Streamlit app
def main():
    st.title("Chatbot")
    st.write("Hello! I'm a chatbot. Ask me anything about the topic in the text file.")
    # Get the user's question
    question = st.text_input("You:")
    # Create a button to submit the question
    if st.button("Submit"):
        # Call the chatbot function with the question and display the response
        response = chatbot(question)
        #print("Chatbot Response" + response)
        st.write("Chatbot: " + data)
        st.write(response)
if __name__ == "__main__":
    main()
